{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-1 Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "I7ubIyty-l-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is a continuous probability distribution that arises frequently in statistics, especially in the analysis of variance (ANOVA) and hypothesis testing when comparing variances. Here are its main properties:\n",
        "\n",
        "1. Asymmetry and Skewness:\n",
        "The F-distribution is right-skewed and asymmetric. It has a peak near zero and extends infinitely towards positive values, with a long tail to the right.\n",
        "\n",
        "2. Non-negative Values:\n",
        "The F-distribution only takes on values greater than or equal to zero. Since it is a ratio of variances, negative values are not possible.\n",
        "\n",
        "3. Degrees of Freedom:\n",
        "The shape of the F-distribution is defined by two sets of degrees of freedom:\n",
        "\n",
        "ùëë1: degrees of freedom for the numerator (variance of the first sample)\n",
        "\n",
        "ùëë2: degrees of freedom for the denominator (variance of the second sample)\n",
        "\n",
        "These degrees of freedom are crucial in determining the shape of the F-distribution. Higher degrees of freedom make the distribution more symmetric and closer to a normal distribution.\n",
        "\n",
        "4. Uses in Hypothesis Testing:\n",
        "\n",
        "The F-distribution is mainly used in ANOVA to test the equality of variances across multiple groups.\n",
        "It is also used in regression analysis to test the significance of a group of predictors, especially when comparing nested models.\n",
        "\n",
        "5. Critical Values:\n",
        "\n",
        "Critical values of the F-distribution are used in hypothesis testing. If the calculated F-statistic is greater than the critical value for a chosen significance level (usually Œ±=0.05), the null hypothesis can be rejected.\n"
      ],
      "metadata": {
        "id": "9Xyp_q8q-uo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-2  In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "Dck3YpqmBqVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is used in several statistical tests, especially when comparing variances or analyzing the relationships between groups. Here's an overview of some common tests that rely on the F-distribution and why it‚Äôs suitable for each:\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "Purpose: ANOVA tests determine if there are significant differences between the means of three or more groups.\n",
        "\n",
        "Why the F-distribution: ANOVA compares group variances (between-group variance to within-group variance). Since the F-distribution is derived from the ratio of variances, it‚Äôs ideal for testing if observed group differences are statistically significant. A large F-statistic suggests the means of the groups are not equal.\n",
        "2. Regression Analysis (Overall Significance)\n",
        "Purpose: In regression analysis, the F-test evaluates whether the overall model is statistically significant, i.e., if at least one predictor variable in the model has an impact on the dependent variable.\n",
        "\n",
        "Why the F-distribution: The F-test compares the variance explained by the model (regression sum of squares) to the unexplained variance (residual sum of squares). Since these variances are based on sums of squared deviations, the F-distribution is well-suited for this comparison.\n",
        "3. Comparing Two Variances\n",
        "Purpose: The F-test can directly compare the variances of two independent populations to determine if they are significantly different.\n",
        "\n",
        "Why the F-distribution: This test uses the ratio of the two sample variances. The F-distribution‚Äôs properties allow us to assess the probability of observing this ratio under the null hypothesis that the variances are equal.\n",
        "4. Testing Nested Regression Models\n",
        "Purpose: This test evaluates whether a simpler (nested) model is sufficient or if a more complex model provides a significantly better fit.\n",
        "\n",
        "Why the F-distribution: By comparing the variance in prediction error between the two models, the F-distribution helps assess whether the additional parameters in the complex model improve the fit enough to justify their inclusion.\n",
        "5. Multivariate Analysis of Variance (MANOVA)\n",
        "Purpose: MANOVA extends ANOVA by testing differences in multiple dependent variables across groups.\n",
        "\n",
        "Why the F-distribution: MANOVA ultimately generates F-statistics as part of the process of comparing variances across groups for each dependent variable, making it well-suited to assess the combined effect across all variables.\n",
        "6. Levene‚Äôs Test for Equality of Variances\n",
        "Purpose: Levene‚Äôs test assesses the homogeneity of variances across groups, a key assumption in ANOVA and other parametric tests.\n",
        "\n",
        "Why the F-distribution: The test uses the F-distribution to determine whether observed differences in variances are significant, thus helping verify that ANOVA assumptions are met.\n",
        "Why the F-Distribution is Appropriate:\n",
        "The F-distribution is used in these tests because:\n",
        "\n",
        "It arises naturally when working with ratios of variances.\n",
        "Its shape (right-skewed and always positive) matches the behavior of variance ratios.\n",
        "It provides critical values and probability thresholds for assessing the significance of observed data."
      ],
      "metadata": {
        "id": "j6eEibKwBzkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-3  What are the key assumptions required for conducting an F-test to compare the variances of two populations?**"
      ],
      "metadata": {
        "id": "diumU3AHCtcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct an F-test to compare the variances of two populations, several key assumptions must be met to ensure the test‚Äôs validity. These assumptions are as follows:\n",
        "\n",
        "1. Normality of Populations\n",
        "Assumption: Both populations from which the samples are drawn should be normally distributed.\n",
        "Importance: The F-test is sensitive to deviations from normality. If the populations are not normally distributed, the F-test may produce inaccurate results, often yielding inflated Type I error rates (false positives). When normality cannot be assumed, alternative tests, like Levene‚Äôs or the Brown-Forsythe test, are preferred for comparing variances.\n",
        "2. Independence of Samples\n",
        "Assumption: The two samples should be independently drawn from the populations.\n",
        "Importance: Independence ensures that the sample values are not related or influenced by each other. If the samples are dependent (e.g., paired or matched samples), the F-test for comparing variances is inappropriate, and other methods should be considered.\n",
        "3. Random Sampling\n",
        "Assumption: Each sample should be a random sample from its respective population.\n",
        "Importance: Random sampling helps ensure that the samples are representative of the populations, minimizing bias. Without random sampling, the test results may not generalize to the populations of interest.\n",
        "4. Ratio of Variances is the Test Statistic\n",
        "Assumption: The test relies on the ratio of the sample variances (S1^2/S2^2) as the F-statistic.\n",
        "Importance: This assumption is inherent in the F-test and forms the basis of the test distribution under the null hypothesis. If the test statistic does not follow the expected ratio, the F-distribution will not be appropriate, invalidating the test results."
      ],
      "metadata": {
        "id": "Op9wB0y6C8l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-4  What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "js8bugHTDgaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of Analysis of Variance (ANOVA) is to determine if there are statistically significant differences among the means of three or more groups. ANOVA is widely used in experimental studies where researchers need to compare multiple groups simultaneously to understand whether any observed differences in group means are due to random variation or reflect true differences in the population.\n",
        "\n",
        "The main aspects of ANOVA and how it differs from a t-test:\n",
        "\n",
        "Purpose and Use of ANOVA\n",
        "\n",
        "1. Comparing Multiple Groups:\n",
        "ANOVA is used when comparing three or more groups to see if there is a difference in means. It helps avoid the problem of increasing the Type I error rate (false positives) that would occur if multiple t-tests were used.\n",
        "\n",
        "2. Partitioning Variability:\n",
        "ANOVA partitions the total variability in the data into two components:\n",
        "Between-group variability: Variability due to the differences in group means.\n",
        "Within-group variability: Variability due to differences within each group, assumed to be due to random error.\n",
        "\n",
        "3. Single Test for Multiple Comparisons:\n",
        "ANOVA provides a single F-test to determine if there is a statistically significant difference across groups, rather than conducting multiple pairwise comparisons as in t-tests.\n",
        "\n",
        "Differences Between ANOVA and t-Test\n",
        "\n",
        "1. Number of Groups\n",
        "ANOVA: Compares three or more groups\n",
        "t-Test: Compares only two groups\n",
        "\n",
        "2. Type of Hypothesis\n",
        "ANOVA: Tests if at least one group mean is different\n",
        "t-Test: Tests if there is a difference between two means\n",
        "\n",
        "3. Error Rate Control\n",
        "ANOVA: Controls for Type I error across multiple groups\n",
        "t-Test: Conducting multiple t-tests increases Type I error\n",
        "\n",
        "4. Test Statistic\n",
        "ANOVA: F-statistic (ratio of between-group to within-group variance)\n",
        "t-Test: t-statistic (difference between means relative to variability)\n",
        "\n",
        "5. Post-hoc Analysis\n",
        "ANOVA: Requires post-hoc tests (e.g., Tukey‚Äôs test) to identify specific group differences\n",
        "t-Test: Not required as it directly compares two groups\n"
      ],
      "metadata": {
        "id": "E-GiuNa_D9es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-5  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**"
      ],
      "metadata": {
        "id": "Zi8NYv2sFK76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A one-way ANOVA is preferred over multiple t-tests when comparing more than two groups because it efficiently tests for differences among group means while controlling for the risk of Type I error (false positives). Here‚Äôs an in-depth look at when and why a one-way ANOVA is used instead of multiple t-tests:\n",
        "\n",
        "When to Use a One-Way ANOVA\n",
        "\n",
        "Comparing More Than Two Groups: A one-way ANOVA is specifically designed for scenarios where there are three or more groups. For example, if you are comparing the mean scores of three different teaching methods on student performance, ANOVA is the best approach.\n",
        "\n",
        "Single Independent Variable: A one-way ANOVA is suitable when there is a single categorical independent variable (factor) that has multiple levels or groups. If there are multiple factors, a two-way or factorial ANOVA might be more appropriate.\n",
        "\n",
        "Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "\n",
        "1. Controls Type I Error Rate:\n",
        "\n",
        "Conducting multiple t-tests increases the likelihood of making a Type I error (rejecting the null hypothesis when it‚Äôs true) because each t-test is associated with its own alpha level (typically 0.05). For example, if you compare three groups (Group A vs. Group B, Group A vs. Group C, Group B vs. Group C), you would conduct three separate t-tests, each with a 5% chance of a Type I error.\n",
        "\n",
        "The more t-tests you conduct, the higher the cumulative Type I error rate, which can quickly exceed acceptable limits. ANOVA, on the other hand, uses a single test to determine if there is any difference among the group means, thus keeping the overall Type I error at the chosen significance level (e.g., 0.05).\n",
        "\n",
        "2. Increased Power and Efficiency:\n",
        "\n",
        "ANOVA is more powerful and efficient because it uses all available data at once to detect any differences among groups. Conducting separate t-tests does not leverage the pooled data as effectively as ANOVA does, potentially missing subtle differences among groups.\n",
        "\n",
        "ANOVA also provides a broader test of the null hypothesis, testing if at least one group mean is different from the others rather than focusing on pairwise comparisons alone.\n",
        "\n",
        "3. Interpretation of Results:\n",
        "\n",
        "ANOVA gives a clear overall result for whether there are any statistically significant differences among the groups. If the ANOVA result is significant, post-hoc tests (e.g., Tukey's HSD) can then be conducted to identify specifically which groups differ. This stepwise approach is simpler and more organized than interpreting multiple individual t-tests.\n",
        "\n",
        "4. Equivalence to Multiple t-Tests for Two Groups:\n",
        "\n",
        "When there are only two groups, a one-way ANOVA and an independent t-test yield the same results, with the F-statistic in ANOVA being equivalent to the square of the t-statistic. However, when there are three or more groups, ANOVA is more appropriate because it provides a single, controlled test."
      ],
      "metadata": {
        "id": "YsWidl3qGlRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-6  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "PjYMZLqPHWlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ANOVA, variance is partitioned into two components: between-group variance and within-group variance. This partitioning allows us to calculate the F-statistic, which assesses whether the means of different groups are significantly different. Here‚Äôs a detailed breakdown of how this process works and how it contributes to the F-statistic.\n",
        "\n",
        "1. Partitioning of Variance in ANOVA\n",
        "Total Variance:\n",
        "The total variance in the data represents the overall variability in all observations, regardless of group membership. In ANOVA, this total variance is broken down into:\n",
        "\n",
        "Between-Group Variance (also known as ‚Äúexplained variance‚Äù): This reflects the variability between the group means and the overall mean. It represents the differences due to the treatment effect or the factor being tested.\n",
        "\n",
        "Within-Group Variance (also known as ‚Äúunexplained variance‚Äù or ‚Äúerror variance‚Äù): This reflects the variability within each group, which is attributed to random error or individual differences within groups.\n",
        "\n",
        "Mathematically, the total sum of squares (SStotal) is partitioned as:\n",
        "\n",
        "SStotal = SSbetween + SSwithin\n",
        "\n",
        "where:\n",
        "\n",
        "SSTotal: is the total sum of squares, representing the total variability of all data points from the grand mean.\n",
        "\n",
        "SSBetween :is the between-group sum of squares, representing the variability due to differences among group means.\n",
        "\n",
        "SSWithin :is the within-group sum of squares, representing the variability within each group.\n",
        "\n",
        "Calculation of the F-Statistic\n",
        "\n",
        "The F-statistic in ANOVA is the ratio of the between-group mean square to the within-group mean square:\n",
        "\n",
        "F = MSbetween/MSwithin\n",
        "\n",
        "This ratio, or F-statistic, measures how much of the total variance is explained by the group differences (between-group variance) relative to the random variance within the groups (within-group variance).\n",
        "\n",
        "Interpreting the F-Statistic\n",
        "\n",
        "If the group means are similar, then the between-group variance will be small compared to the within-group variance, leading to an F-statistic close to 1. This suggests that any observed differences are likely due to chance.\n",
        "\n",
        "A large F-statistic (much greater than 1) indicates that the between-group variance is substantially greater than the within-group variance, suggesting that the differences among group means are unlikely to be due to random chance and are statistically significant."
      ],
      "metadata": {
        "id": "oi9O0Sj5Hd5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-7 Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "u4j9XX8aLrwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classical (frequentist) approach and the Bayesian approach to ANOVA differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here‚Äôs a comparison of the key differences:\n",
        "\n",
        "1. Handling Uncertainty\n",
        "\n",
        "Classical (Frequentist) ANOVA:\n",
        "In the frequentist approach, uncertainty is handled through sampling distributions and p-values. The approach assumes that data are drawn from a random sample, and repeated sampling would yield results centered around the true population parameters.\n",
        "Uncertainty in hypothesis testing is managed by setting a significance level (e.g.,Œ±=0.05), which represents the probability of rejecting a true null hypothesis (Type I error).\n",
        "Confidence intervals provide a range of plausible values for estimates but do not represent probability distributions over those estimates.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "Bayesian ANOVA handles uncertainty by treating parameters as random variables with probability distributions that represent belief about their values, given prior knowledge and observed data.\n",
        "Uncertainty is incorporated through posterior distributions, which are updated beliefs about the parameters after observing the data. This distribution captures a range of possible parameter values with associated probabilities.\n",
        "Bayesian credible intervals offer a direct probabilistic interpretation: a 95% credible interval means there is a 95% probability that the parameter lies within that interval, given the observed data.\n",
        "\n",
        "2. Parameter Estimation\n",
        "\n",
        "Classical ANOVA:\n",
        "The frequentist approach estimates parameters (such as group means) using point estimates, which are derived directly from the observed data.\n",
        "It does not assume prior knowledge about the parameters and relies on maximum likelihood estimation (MLE) to calculate these point estimates.\n",
        "Variability in parameter estimates is assessed using sampling distributions and standard errors, which assume repeated sampling.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "Bayesian ANOVA incorporates prior distributions on parameters, reflecting any prior knowledge or assumptions about the likely values of these parameters.\n",
        "The parameter estimates are derived from the posterior distribution, which combines the prior distribution with the observed data.\n",
        "Bayesian estimation provides a posterior mean or posterior median as a central tendency measure and allows for a fuller representation of uncertainty around each parameter through its posterior distribution.\n",
        "\n",
        "3. Hypothesis Testing\n",
        "\n",
        "Classical ANOVA:\n",
        "In frequentist ANOVA, hypothesis testing focuses on whether there is sufficient evidence to reject the null hypothesis (no difference between group means) at a specified significance level.\n",
        "The F-test is used to compare between-group variance to within-group variance, producing an F-statistic and a p-value. A p-value below the significance threshold (e.g., 0.05) leads to rejecting the null hypothesis.\n",
        "Frequentist ANOVA does not quantify the probability that the null hypothesis is true or false; it only assesses if the data provide enough evidence against it.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "Bayesian hypothesis testing assesses the probability of different hypotheses given the observed data. Instead of rejecting or failing to reject the null hypothesis, Bayesian ANOVA calculates a Bayes factor or the posterior probability of competing hypotheses.\n",
        "A Bayes factor quantifies the strength of evidence for one hypothesis over another, often comparing the null hypothesis to an alternative hypothesis.\n",
        "Bayesian methods provide a nuanced interpretation, allowing the analyst to state the probability of each hypothesis being true, given the data.\n",
        "\n",
        "4. Interpretation of Results\n",
        "\n",
        "Classical ANOVA:\n",
        "Results are interpreted in terms of statistical significance, with conclusions based on whether the p-value is below a pre-set alpha level. If the p-value is significant, it suggests that at least one group mean differs from the others.\n",
        "Frequentist interpretations do not express probabilities about the hypotheses themselves but focus on whether the observed data is extreme enough to reject the null hypothesis.\n",
        "\n",
        "Bayesian ANOVA:\n",
        "Results are interpreted as probabilities for the hypotheses and parameters directly. Bayesian interpretation allows stating that there is a specific probability (e.g., 90%) that a given hypothesis or parameter value is true, given the data.\n",
        "Bayesian methods provide more flexibility in incorporating prior knowledge and in making direct probability statements about hypotheses and parameters.\n",
        "\n",
        "Summary of Key Differences\n",
        "\n",
        "1. Uncertainty\n",
        "Classical ANOVA: Managed through p-values and significance levels\n",
        "Bayesian ANOVA: Managed through posterior distributions\n",
        "\n",
        "2. Parameter Estimation\n",
        "Classical ANOVA: Point estimates, relies on observed data only\n",
        "Bayesian ANOVA: Uses prior + observed data to estimate posterior distribution\n",
        "\n",
        "3. Hypothesis Testing\n",
        "Classical ANOVA: F-test, focuses on rejecting/failing to reject null hypothesis\n",
        "Bayesian ANOVA: Bayes factor/posterior probability, gives direct probability for hypotheses\n",
        "\n",
        "4. Interpretation\n",
        "Classical ANOVA: Focus on statistical significance and p-values\n",
        "Bayesian ANOVA: Probabilistic interpretation with credible intervals and posterior probabilities"
      ],
      "metadata": {
        "id": "Sd4-cBImLzdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-8  Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.**"
      ],
      "metadata": {
        "id": "yzsO6llFPQ01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Gv1aMA99my",
        "outputId": "a46970aa-0483-4f68-f00e-0686becc08bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.24652429950266952)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "income_A = np.array([48, 52, 55, 60, 62])\n",
        "income_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "var_A = np.var(income_A, ddof=1)\n",
        "var_B = np.var(income_B, ddof=1)\n",
        "\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "df_A = len(income_A) - 1\n",
        "df_B = len(income_B) - 1\n",
        "p_value = stats.f.cdf(F_statistic, df_A, df_B) if F_statistic < 1 else 1 - stats.f.cdf(F_statistic, df_A, df_B)\n",
        "\n",
        "F_statistic, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-9 Conduct a one-way ANOVA to test whether there are any statistically significant differences in**\n",
        "**average heights between three different regions with the following data**\n",
        "\n",
        "Region A: [160, 162, 165, 158, 164']\n",
        "\n",
        "Region B: [172, 175, 170, 168, 174']\n",
        "\n",
        "Region C: [180, 182, 179, 185, 183']\n",
        "\n",
        "**Task: Write Python code to perform the one-way ANOVA and interpret the results**\n",
        "\n",
        "**Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.**"
      ],
      "metadata": {
        "id": "nCy_YvByQhv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMtDe83AP9qS",
        "outputId": "9b870780-ce4c-4dac-e30c-4aed4847d42f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FPl5jgdRAe0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}